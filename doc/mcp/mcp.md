掌握模型上下文协议 (MCP)：安装、使用与开发综合指南
1. 模型上下文协议 (MCP) 简介
模型上下文协议 (Model Context Protocol, MCP) 正在迅速成为人工智能领域一项基础性的技术。它旨在解决大型语言模型 (LLM) 与外部工具、数据源和服务集成时面临的复杂性问题。
1.1. MCP 是什么？“AI领域的USB-C”类比
数据点： MCP 是由 Anthropic 推出的一项开放标准、开源框架，旨在标准化 AI 模型（LLM）与外部工具、系统和数据源集成及共享数据的方式 1。它常被称为“AI 应用的 USB-C” 3。
背景与重要性： 这个在众多资料中出现的类比 2 至关重要，因为它直观地传达了 MCP 的核心价值主张：为 AI 连接各种能力提供一个通用的、标准化的接口，就像 USB-C 标准化了外围设备的连接一样。这解决了在快速发展的人工智能领域中，专有集成方案普遍存在的一大痛点。
“开放标准”的含义： MCP 的“开放标准”特性 1 不仅仅是一个标签，它意味着致力于社区驱动的演进、超越单一供应商生态系统的互操作性，并且如果得到广泛采用，还可能带来长期的稳定性和信任。这与供应商锁定的集成方法形成了对比。MCP 被反复描述为一个“开放标准” 1，由 Anthropic 发起，但 OpenAI 和 Google DeepMind 也已采纳或支持它 2。成为“开放标准”的实际后果是什么？首先，开放标准鼓励多个供应商和开发者构建兼容的工具，这可以防止供应商锁定，并允许用户混合和匹配来自不同提供商的 AI 模型、应用程序和 MCP 服务器。其次，开放标准通常会培育一个社区，为规范的制定做出贡献，确保其满足更广泛的需求并适应新的挑战 4；GitHub 上关于协议演进的讨论 11 也支持了这一点。最后，由多个主要参与者和活跃社区支持的标准更有可能获得广泛采用并持续存在，使其成为长期开发项目中比可能被弃用的专有解决方案更安全的选择。因此，“开放标准”标志着一项战略选择，旨在培育一个广泛、可互操作的生态系统，而不是一个封闭的、专有的生态系统。这是其关键的差异化因素，也是其迅速获得行业巨头早期采纳的主要原因。
1.2. 解决的核心问题：M x N 集成挑战
数据点： 在 MCP 出现之前，开发人员面临着“N×M”（或 M x N）的数据集成问题，需要为每个 AI 应用程序 (M) 与每个外部工具/数据源 (N) 构建自定义连接器 2。MCP 将此问题转化为 M+N 问题 9。
背景与重要性： 这直接解决了困扰 AI 集成的巨大开发开销和碎片化问题。M+N 解决方案凸显了 MCP 的效率：为您的应用程序构建一个 MCP 客户端接口，为您的工具构建一个 MCP 服务器接口，它们就可以与任何其他符合 MCP 的组件进行互操作。
经济和可扩展性影响： M+N 解决方案具有深远的经济影响。它极大地减少了将 AI 与新工具和数据源集成所需的开发时间、成本和维护工作。这加速了创新，并使得更广泛的组织（而不仅仅是那些拥有大量工程资源的组织）能够实现复杂的 AI 集成。除了技术上的简化，解决这个问题还有更广泛的后果。首先是成本降低：更少的自定义集成意味着开发人员在“打通管道”上花费的时间更少，直接降低了劳动力成本。维护也得到了简化，因为对工具的更改只需要更新其 MCP 服务器，而不需要更新使用该工具的每个应用程序。其次是更快的上市时间：新的 AI 功能或工具集成可以更快地推出。如果一个 AI 应用程序支持 MCP，它可以立即利用任何新上线的 MCP 服务器。再次是普惠性：小型公司或个人开发者可以更容易地构建强大的、集成的 AI 解决方案，因为他们不需要为每个连接都重新发明轮子，而是可以利用不断增长的 MCP 服务器生态系统 12。最后是集成的可扩展性：随着 AI 模型和工具数量的爆炸式增长，M x N 的方法变得不可持续。MCP 为管理这些集成提供了一个可扩展的架构模式。因此，MCP 的 M+N 解决方案不仅仅是一种工程上的便利，它还是在整个行业内实现更广泛、更快、更经济高效的 AI 采纳和创新的战略推动者。
1.3. 主要优势：标准化、互操作性、可扩展性、注重安全
数据点： 标准化和互操作性 4，灵活性和供应商无关的设计 4，不断增长的生态系统和可重用性 4，增强的上下文感知能力 6，动态工具发现 6，改进的安全性和访问控制 6。
背景与重要性： 这些优势共同使 MCP 具有吸引力。标准化是基础，从而实现互操作性。不断增长的生态系统提供了直接的价值。协议的可扩展性设计 14 确保了它能够适应未来的需求。
标准化与创新之间的平衡： 虽然标准化是核心优势，但如果过于僵化，有时会扼杀创新。MCP 的渐进式功能添加和能力协商的设计原则 14 似乎通过允许一个最小核心和可选的高级功能来解决这个问题。这种平衡对于长期成功至关重要。MCP 如何在稳定标准的需求与创新和添加新能力的需求之间取得平衡？首先，MCP 有一个所有实现都必须支持的基础协议和生命周期管理组件 15，这确保了一个共同的基础。其次，其他组件（服务器功能、客户端功能）可以根据具体需求选择实现 15。这允许了专业化。再次，初始化阶段包括客户端和服务器交换能力信息 16，这使它们能够了解对方支持哪些功能。最后，“功能可以逐步添加到服务器和客户端……协议设计用于未来的可扩展性。向后兼容性得以保持。” 14 这明确说明了设计目标。因此，MCP 试图实现“稳定的灵活性”。核心是标准化的，但协议允许可选功能和协商，从而在不破坏依赖核心的现有实现的情况下实现创新。这是一种成熟的协议设计方法。
1.4. 当前状态与采纳情况 (Anthropic, OpenAI, Google等)
数据点： 由 Anthropic 于 2024 年末（2024年11月）推出 2。OpenAI 于 2025 年 3 月在其 ChatGPT、Agents SDK 和 Responses API 中采用 MCP 2。Google DeepMind 于 2025 年 4 月确认 Gemini 将支持 MCP 2。其他采用者包括 Zed、Sourcegraph、Replit、Microsoft 2。
背景与重要性： 在 Anthropic 发布后不久，OpenAI 和 Google DeepMind 等主要 AI 参与者的迅速采纳，标志着强劲的行业势头，并验证了 MCP 方法的有效性。这极大地增加了 MCP 成为事实标准的可能性。
竞争对手采纳的战略意义： 主要竞争对手（OpenAI, Google）迅速采纳由另一竞争对手（Anthropic）发起的协议，这一点非常重要。这表明在 AI 代理领域，对于互操作性的迫切需求已达成共识，可能将生态系统增长置于在这一特定层面追求专有优势之上。这可能是一种“竞合”（竞争者之间的合作）动态。大型 AI 实验室都面临着同样的 M x N 集成挑战。一个通用标准通过减少重复工作和扩大工具及代理的潜在市场，使所有人受益。AI 代理的价值随着它们可以连接的工具和数据源数量的增加而增加。标准化协议加速了这个生态系统的增长，惠及所有参与者，这是一个“水涨船高”的局面。用户可能更喜欢能够轻松与各种现有工具和服务集成的 AI 系统。支持像 MCP 这样的通用标准在可用性和灵活性方面可能成为一种竞争优势。通过标准化“管道”，AI 公司可以更专注于其核心差异化因素，例如其 LLM 的能力或其代理平台的独特功能。因此，竞争对手的迅速采纳表明，共享互操作性层所带来的感知利益，超过了在这一特定领域试图建立专有协议的潜在优势。这表明业界已成熟地认识到代理生态系统需要共同的基础设施才能蓬勃发展。
2. 理解 MCP 架构与核心概念
MCP 的架构设计旨在提供灵活性和可扩展性，同时确保不同组件之间的清晰职责划分。
2.1. MCP 三要素：主机 (Host)、客户端 (Client) 与服务器 (Server)
数据点：
主机 (Hosts)： 面向用户的 LLM 应用程序（例如 Claude Desktop、Zed 或 Cursor 等 IDE、AI 工具），负责发起连接并协调流程 5。
客户端 (Clients)： 主机内部的组件，负责管理与特定 MCP 服务器的一对一通信 5。在某些上下文中常与主机互换使用，但技术上有所区别。
服务器 (Servers)： 通过 MCP 暴露能力（工具、资源、提示）的外部程序/服务 5。可以是本地的 (stdio) 或远程的 (HTTP) 3。
背景与重要性： 这种清晰的关注点分离是 MCP 设计的基础。主机处理用户交互和高级协调，客户端管理各个服务器连接的协议细节，而服务器则专注于提供特定能力。这种模块化是可组合性和易开发性的关键 14。
“客户端”的模糊性及其影响： “主机”和“客户端”的可互换使用 9 可能会引起混淆。然而，技术上的区别很重要：单个主机应用程序可以管理多个客户端实例，每个实例连接到不同的 MCP 服务器。这种一对多的主机到客户端到服务器的映射，使得 AI 应用程序能够同时利用多种工具和数据源。文献 9 指出，“很多内容互换使用‘客户端’和‘主机’。从技术上讲，主机是面向用户的应用程序，而客户端是主机应用程序中管理与特定 MCP 服务器通信的组件。” 14 也对它们进行了区分。这种区分之所以重要，是因为单个 AI 应用程序（主机）通常需要与多个专用服务（例如文件系统服务器、数据库服务器、GitHub 服务器）通信。这些连接中的每一个都由主机内的专用客户端组件管理。每个客户端都与其各自的服务器保持一个隔离的、有状态的会话 14。这意味着主机不需要为每个连接管理低级协议状态；它将此任务委托给客户端组件。主机可以通过协调其各个客户端实例来组合来自多个服务器的能力。例如，它可能使用一个客户端从数据库服务器获取数据，并使用另一个客户端通过 Slack 服务器发送该数据。因此，主机-客户端-服务器的区别，特别是主机内多个客户端的角色，对于理解 MCP 如何实现可组合性以及允许单个 AI 应用程序利用丰富多样的独立服务生态系统至关重要。认识到这一点可以防止对架构的过度简化。
2.2. 通信骨干
2.2.1. 协议层：JSON-RPC 2.0
数据点： 所有消息必须遵循 JSON-RPC 2.0 规范 15。定义了请求（必须包含 ID，ID 非空且唯一）、响应（包含相同 ID，必须包含 result 或 error 之一）和通知（无 ID，无响应）15。批量发送 MAY 支持，批量接收 MUST 支持 15。
背景与重要性： JSON-RPC 是一种轻量级、简单且广泛支持的协议。它的使用降低了实现的门槛。严格的消息格式确保了可靠的通信。
选择 JSON-RPC 的考量： 选择 JSON-RPC 2.0 带来了诸多好处，例如人类可读性（JSON）、多种语言的现有库以及简单的请求/响应/通知模型。然而，这也意味着 MCP 继承了 JSON-RPC 的任何局限性，例如其基于文本的特性（除非进行编码，否则对于大型二进制数据可能效率较低）以及需要在其结构内仔细设计错误处理。将 TypeScript 模式作为“事实来源” 15 有助于减少歧义并在定义层面确保类型安全。JSON-RPC 本身是无状态的，但 MCP 在 JSON-RPC 消息之上建立了有状态的连接 13，状态在 MCP 层进行管理。批量处理要求（必须接收）也表明了对潜在性能需求的认识。总而言之，JSON-RPC 2.0 是 MCP 的一个务实选择，优先考虑了易实现性、可读性和广泛的兼容性。协议设计者似乎通过强大的模式定义和有状态的会话管理对其进行了增强，以满足 MCP 的特定需求。
2.2.2. 传输层：Stdio、HTTP with SSE、Streamable HTTP
数据点：
Stdio (标准输入/输出)： 使用标准输入/输出。非常适合本地进程 3。延迟低，依赖操作系统级别的安全性 24。
HTTP with SSE (服务器发送事件)： 客户端到服务器使用 HTTP POST，服务器到客户端使用 SSE 3。用于远程通信。SSE 是单向的（服务器到客户端）。
Streamable HTTP： MCP 规范中定义的一种较新的传输方式，也用于远程服务器 3。使用 HTTP POST 和 GET，服务器可以选择使用 SSE 进行流式传输。这似乎是 HTTP+SSE 的演进，提供了更大的灵活性（例如可恢复性 22）。
背景与重要性： 传输方式的选择取决于部署场景（本地与远程、安全需求、网络环境）。这种灵活性对于 MCP 的适应性至关重要。
HTTP 传输的演进与安全影响： 同时存在 "HTTP with SSE" 和 "Streamable HTTP" 3 表明 MCP 处理远程通信的方式有所演进。"Streamable HTTP" 似乎更为健壮，可能解决了简单 SSE 方法在服务器到客户端流式传输方面的局限性。对于远程传输，安全性（TLS、来源验证、身份验证）变得至关重要 15。Stdio 的安全性更多地依赖于操作系统级别的进程隔离 24。基础的 SSE 纯粹是服务器到客户端的。对于双向通信，它通常与来自客户端到服务器的常规 HTTP POST 配对，这在 "HTTP with SSE" 中有所体现。"Streamable HTTP" 22 似乎规范了这种双向 HTTP 通信，允许使用 GET 打开 SSE 流，使用 POST 向服务器发送消息。它还引入了诸如可恢复性和通过 HTTP 状态码（例如 202 Accepted）对响应/通知进行特定处理等概念。这表明存在一个更成熟、功能更丰富的 HTTP 传输。Stdio 的安全性主要继承自操作系统进程模型；如果主机启动一个 stdio 服务器，它通常是一个具有类似权限的子进程或由主机沙箱化 24。授权更多地关乎主机允许服务器进程执行什么操作，而不是网络身份验证。基于 HTTP 的传输是网络传输，因此标准的 Web 安全实践至关重要：使用 TLS 进行加密 16，验证 Origin 头部 22，以及一个正式的授权框架 15，通常基于 OAuth 2.1。传输层提供了针对不同部署场景的选择。Stdio 对于本地使用而言简单且安全。HTTP 传输则满足远程场景的需求，其中 "Streamable HTTP" 是更全面且未来可能更受青睐的标准。本地和远程传输的安全考虑因素差异显著，需要开发人员进行适当的选择和配置。
表1：MCP 传输机制比较

传输类型
主要用例
通信方式
主要特性
安全考虑因素
SDK 支持说明
Stdio
本地进程间通信
标准输入/输出
低延迟、简单、依赖操作系统级安全
进程隔离、主机权限控制
多数 SDK 均提供支持，例如 Python MCPServerStdio 3
HTTP with SSE (传统)
远程通信
HTTP POST (客户端->服务器), SSE (服务器->客户端)
适用于需要服务器推送的场景
TLS 加密、身份验证、CORS
一些早期或简单实现中可见 3
Streamable HTTP (标准)
远程通信
HTTP POST/GET, 可选 SSE 流
更健壮、支持双向、可恢复性、明确的 HTTP 状态码处理 22
TLS 加密、Origin 验证、OAuth 2.1 授权框架 15
推荐用于新的远程实现，SDK 正在逐步支持 3




*   **2.2.3. 消息类型：请求、响应、通知、批量处理**
    *   **数据点：**
        *   **请求 (Requests)：** 客户端到服务器或反之。必须包含字符串/数字 ID (非空，在会话中请求者唯一)。结构: `{ jsonrpc: "2.0"; id: string | number; method: string; params?: {... }; }` [15, 16]。
        *   **响应 (Responses)：** 对请求的回复。必须包含与请求相同的 ID。必须设置 `result` 或 `error` 之一，且不能同时设置。结构: `{ jsonrpc: "2.0"; id: string | number; result?: {... } error?: { code: number; message: string; data?: unknown; } }` [15, 16]。
        *   **通知 (Notifications)：** 单向消息。不得包含 ID。接收方不得发送响应。结构: `{ jsonrpc: "2.0"; method: string; params?: {... }; }` [15, 16]。
        *   **批量处理 (Batching)：** JSON-RPC 允许将多个请求/通知批量打包在一个数组中发送。MCP 实现 MAY 支持发送批量，但 MUST 支持接收批量 [15]。
    *   **背景与重要性：** 这些标准的 JSON-RPC 消息结构构成了所有 MCP 通信的基础。理解它们在 MCP 中的特定约束（例如 ID 规则、批量处理支持）对于正确实现至关重要。
    *   **非对称的批量处理支持：** 要求服务器*始终*支持接收批量，而客户端*可以*支持发送批量 [15]，这对服务器实现提出了更高的要求，需要处理可能更复杂的输入。这种设计选择可能优先考虑了服务器的健壮性和与各种客户端的互操作性，其中一些客户端可能会利用批量处理来提高效率。服务器需要编码以解析和处理请求/通知数组，如果 JSON-RPC 规范允许，则可能需要处理批处理中的部分成功/失败（尽管 MCP 响应是针对每个请求 ID 的单个响应）。如果客户端不需要批量处理，则可以更简单。如果需要，它们可以使用它进行优化（例如，在一个网络数据包中发送多个工具调用或资源请求）。这确保了任何客户端，无论是否进行批量处理，都可以与任何服务器通信。这种不对称性反映了一种设计，该设计倾向于服务器的完整性，以适应各种客户端行为和优化策略。


2.3. 基本能力
2.3.1. 工具 (Tools)：赋能行动
数据点： 由服务器暴露的可执行功能，LLM 可以在用户批准的情况下调用这些功能来执行操作或检索计算数据 6。工具使用名称、描述、inputSchema (JSON Schema) 和可选的注释进行定义 28。客户端通过 tools/list 发现工具，通过 tools/call 调用工具 28。
背景与重要性： 工具是将 LLM 能力扩展到文本生成之外，允许它们与世界互动的主要机制。清晰的定义和发现机制至关重要。
工具注释与用户体验 (UX)： 工具注释 28（如 title）是用于 UX 的“提示”，而非用于安全。这种分离很重要：核心工具定义（名称、模式）供 LLM 和协议使用，而注释则帮助主机应用程序以更易于理解的方式向用户呈现工具（例如，用于批准对话框）。这表明工具元数据采用分层方法。文献 28 讨论了 tool.annotations，它“提供有关工具行为的附加元数据，帮助客户端理解如何呈现和管理工具。这些注释是提示……但不应依赖它们来做出安全决策。” description 和 inputSchema 主要供 LLM 理解如何使用该工具。注释似乎通过主机应用程序的 UI 面向人类用户。注释可以提供“不影响模型上下文的 UX 特定信息” 28。这允许主机构建更丰富的 UI 来发现、选择和批准工具，而不会使 LLM 需要的信息变得混乱。明确指出注释“不应依赖它们来做出安全决策” 28 是一项关键的安全指南。安全性必须基于受信任服务器的实际工具实现和主机中强大的同意机制，而不仅仅是描述性元数据。因此，工具注释是一种经过深思熟虑的设计元素，它将供机器使用（LLM）的元数据与供人类呈现（UX）的元数据分开，同时也强调描述性元素不能替代安全措施。
2.3.2. 资源 (Resources)：提供数据
数据点： 由服务器为 LLM 上下文暴露的结构化、只读数据流/类文件数据（例如文件、日志、API 响应、数据库记录）6。不应执行大量计算或产生副作用 31。
背景与重要性： 资源使 LLM 基于特定相关数据，从而提高其输出的准确性和相关性。只读、无副作用的特性简化了它们的使用和推理。
资源与工具在数据检索方面的区别： 虽然工具可以检索数据，但资源专门用于提供现有的、结构化的数据，而无需进行计算。这种区别有助于能力建模：如果只是获取数据，则为资源；如果涉及处理或操作，则为工具。这可能会影响 LLM 决定如何获取信息。资源是“只读数据源” 9 并且“不应执行大量计算或产生副作用” 31。工具是“可执行函数” 9 并且“期望执行计算并产生副作用” 31。如果数据相对静态或可直接访问（如文件内容或数据库记录），则资源是合适的。如果检索数据需要大量计算、使用参数调用另一个 API 或执行生成数据的操作，则工具更合适。例如，根据用户输入查询数据库的“搜索”功能将是一个工具，而对特定表模式的直接访问可能是一个资源。这种区别可以指导 LLM。如果它需要原始数据，它可能会寻找资源。如果它需要执行操作来获取数据，它将寻找工具。因此，即使两者都可能涉及数据，资源和工具的分离为 LLM 和设计 MCP 服务器的开发人员提供了一个更清晰的语义模型。它区分了被动数据提供和主动数据生成/检索。
2.3.3. 提示 (Prompts)：引导交互
数据点： 可重用的指令模板或预定义工作流程，用于常见任务，引导用户、AI 模型和可用能力之间的交互 6。
背景与重要性： 提示有助于标准化常见操作，并使用户和 LLM 更容易通过服务器的能力实现特定目标。它们封装了使用服务器功能的最佳实践。
提示作为打包的专业知识： MCP 提示可以被视为服务器开发人员将其关于如何最佳使用其服务器的工具和资源来完成特定任务的专业知识打包的一种方式。这减轻了客户端/主机开发人员或最终用户的一些提示工程负担。提示是“可重用模板”，用于“引导交互”并帮助“完成特定任务” 6。与主机或用户只是自己制作提示来使用服务器的工具/资源相比，服务器定义的提示的独特价值是什么？服务器开发人员可能最了解提示 LLM 以针对给定任务与其特定工具和资源进行交互的最有效方法（例如，“使用我们特定的文档资源格式和摘要工具总结文档”）。服务器定义的提示确保了可能连接到该服务器的不同客户端在处理常见任务时采用一致的方法。它们降低了用户/主机的门槛，用户/主机可以利用这些预定义的工作流程，而无需成为该特定服务器功能细微之处的专家。例如，GitHub MCP 服务器可能会提供一个“从问题创建拉取请求”的提示，该提示确切知道要调用哪些工具（例如，get_issue_details、create_branch、commit_changes、create_pull_request）以及如何为每个步骤构建 LLM 请求。因此，MCP 提示不仅仅是文本模板；它们是一种机制，使服务器能够提供预先打包的、专家指导的工作流程，从而增强可用性并确保更有效地利用其能力。
2.3.4. 采样 (Sampling)：服务器发起的 LLM 交互
数据点： 服务器发起的、请求客户端/主机执行 LLM 交互的请求 7。支持递归操作，服务器从 LLM 请求补全。客户端审查请求，可以修改，对 LLM进行采样，审查补全，然后将结果返回给服务器 33。用户必须明确批准采样请求 13。
背景与重要性： 采样是一项强大的功能，它允许服务器通过利用主机的 LLM 来表现得更主动并参与更复杂的、代理式的行为。这对于服务器需要 LLM 的推理或生成能力才能继续进行的工​​作流程至关重要。
采样中的安全与控制： 采样流程（33：服务器请求 -> 客户端审查/修改 -> 客户端对 LLM 采样 -> 客户端审查 -> 客户端返回）将重要的控制点置于客户端（并因此延伸至用户）。这对于安全以及防止失控的代理行为或滥用 LLM 至关重要，因为服务器并不直接调用 LLM。用户同意要求 13 进一步强化了这一点。“用户必须明确批准任何 LLM 采样请求。用户应控制：是否进行采样。将发送的实际提示。服务器可以看到哪些结果。” 13。AWS 博客 33 详细介绍了一个多步骤流程，其中客户端在 LLM 调用之前和之后有多次审查和修改的机会。为什么对采样如此强调这种客户端控制和用户批准的程度？如果服务器可以直接且不透明地导致主机的 LLM 执行任意提示，则可能导致：提示注入/劫持：恶意服务器可能试图使 LLM 执行意外操作。资源滥用：服务器可能进行过多的 LLM 调用，给用户带来成本或达到速率限制。数据泄露：服务器可能制作提示以试图提取 LLM 在主机上下文中可能访问到的敏感信息。控制点确保用户（通过主机应用程序）仍然掌控其 LLM 的使用方式、发送的提示以及与服务器共享的信息。因此，精心设计的、带有强制性客户端/用户检查点的采样流程是一项关键的安全和信任功能。它允许强大的服务器发起的代理行为，同时减轻了让外部服务器直接控制用户 LLM 所带来的风险。这是 MCP 对外部能力采取“信任但验证”方法的关键方面。
2.4. 连接生命周期：初始化、消息交换、终止
数据点：
初始化 (Initialization)： 客户端发送 initialize（包含协议版本、能力），服务器响应（包含其版本、能力），客户端发送 initialized 通知 16。
消息交换 (Message Exchange)： 初始化后，支持请求-响应和通知 16。
终止 (Termination)： 任一方均可通过 close()、传输断开或错误条件终止连接 16。
背景与重要性： 这个定义的生命周期确保了有序的连接建立、能力协商和关闭，这对于有状态、可靠的通信至关重要。
能力协商作为可扩展性的基石： 初始化期间的能力协商 14 不仅仅是一次握手。它是 MCP 可扩展性和前向/后向兼容性的基础。客户端和服务器仅使用它们都理解和支持的功能，从而允许协议在不破坏旧实现的情况下发展。初始化包括交换“能力” 16。文献 14 指出，“已实现的服务器功能必须在其服务器的能力中声明……这种能力协商确保客户端和服务器清楚地了解支持的功能，同时保持协议的可扩展性。” 如果协议要求所有实现都支持所有功能，那么添加任何新功能都会对每个人造成破坏性更改。能力协商避免了这种情况。新的客户端可以与旧的服务器通信（反之亦然）。它们将基于它们共同支持的能力子集进行操作。新的客户端可能无法将其最新的功能与旧的服务器一起使用，但基本的交互仍然可以工作。服务器或客户端可以选择仅实现与其需求相关的 MCP 功能子集 15，并通过能力进行声明。一个简单的文件服务器可能不需要“采样”能力。因此，能力协商是一种关键的架构模式，它使 MCP 既稳定（核心功能）又可演进（新的可选功能）。它防止协议变得脆弱，并鼓励逐步采用新功能。
2.5. 授权框架概述
数据点： MCP 为基于 HTTP 的传输提供了一个授权框架（SHOULD 遵循），但不适用于 STDIO（SHOULD 从环境中检索凭据）15。也支持自定义策略 15。基于 OAuth 2.1、RFC8414、RFC7591、RFC9728 26。序列图涉及 AS 发现、OAuth 流程、令牌使用 26。
背景与重要性： 对于远程服务器，尤其是那些处理敏感数据或操作的服务器，强大的授权机制是必不可셔的。将其建立在 OAuth 标准之上，可以促进与现有身份系统的互操作性。
授权中的复杂性与安全性权衡： 虽然基于 OAuth 2.1 26 进行标准化有利于安全性和互操作性，但正确实施 OAuth 可能很复杂。规范通过说明它实现了“其功能的一个选定子集，以确保安全性和互操作性，同时保持简单性” 26 来承认这一点。这是一个务实的权衡。“明确禁止令牌传递”规则 25 是一项关键的安全措施，可防止混淆代理问题并保持问责制。MCP HTTP 授权基于 OAuth 2.1 及相关 RFC 15。文献 26 指出它使用“选定子集”以求“简单性”。文献 25 明确禁止“令牌传递”。这些选择对开发人员和安全性有何影响？使用 OAuth 利用了一个经过充分审查的行业标准框架来进行委托授权。这通常比发明自定义身份验证机制要好得多。完整的 OAuth 2.1 对于授权服务器 (AS) 和依赖方（在这种情况下是 MCP 服务器/客户端）来说可能都很复杂。“选定子集”旨在减轻这种负担，同时保留核心安全优势。如果 MCP 服务器只是将其从客户端收到的令牌传递给下游 API 而没有进行适当的验证（受众、范围），则可能允许客户端滥用 MCP 服务器对该下游 API 的信任。MCP 服务器将无法审计或控制客户端通过其凭据执行的操作。禁止这样做 25 迫使 MCP 服务器成为下游 API 的实际 OAuth 客户端，使用其自己的凭据或将传入令牌交换为特定于其需求的令牌。这保持了清晰的信任边界和可审计性。因此，MCP 的授权框架试图取得平衡：利用强大的 OAuth 标准，但在可能的情况下对其进行简化。禁止令牌传递是一项关键的安全设计选择，虽然对于某些代理服务器可能增加了一个步骤，但显著增强了安全性和问责制。与其他受 OAuth 保护的 API 交互的 MCP 服务器的开发人员必须正确理解和实现这一点。
3. 搭建 MCP 环境（安装）
成功开发和使用 MCP 的第一步是正确配置您的开发环境并安装必要的软件开发工具包 (SDK)。
3.1. 先决条件（通用和特定语言）
通用数据点： 熟悉 LLM 29。能够访问 LLM（例如 Claude，或与 OpenAI Agents SDK 等客户端 SDK 兼容的 LLM 3）。
特定语言数据点：
Python：Python 3.10+ 29，使用 uv 进行环境管理 29。MCP Python SDK 1.2.0+ 29。
Node.js/TypeScript：已安装 Node.js 29（用于服务器），31（用于 SDK）。需要 npm 或 yarn。
Java：Java 17+，Maven 3.6+ 34。运行测试需要 Docker 和 npx 35。
C#：.NET 环境，NuGet 包管理器 36。
背景与重要性： 确保正确的开发环境和依赖项就位是成功进行 MCP 开发的第一步。
Python 中 uv 的兴起： Python 快速入门指南 29 推荐使用 uv (来自 Astral) 进行环境和包管理，这一点值得注意。uv 是一个较新的、基于 Rust 的高速工具，旨在取代 pip 和 venv 工作流程。它的引入表明 MCP Python 生态系统正在拥抱现代、高性能的工具。Python 服务器快速入门 29 明确指示安装和使用 uv。与 pip 相比，uv 在依赖项解析和包安装方面以其显著的速度提升而闻名。这表明 MCP Python SDK 的开发和文档是最新的，并采用了 Python 生态系统中较新的最佳实践。对于从事多个 Python 项目的开发人员来说，更快的环境设置可以显著改善生活质量。因此，推荐使用 uv 是一个微妙但强烈的信号，表明 MCP Python 开发体验旨在实现现代化和高效，并利用 Python 社区的前沿工具。
3.2. 官方 SDK 概览
数据点： 提供 Python、TypeScript、Java、C# 的 SDK 2。GitHub 仓库链接：
TypeScript: modelcontextprotocol/typescript-sdk (官方27) - 注意：31 指向 i-am-bee/mcp-typescript-sdk 和 smithery-ai/mcp-ts-sdk，声称是官方或复刻版本，但 37 (组织页面) 将 modelcontextprotocol/typescript-sdk 列为置顶的官方版本。此处以组织置顶版本为准。
Python: modelcontextprotocol/python-sdk (官方13)。
Java: modelcontextprotocol/java-sdk (官方, 与 Spring AI 共同维护35)。34 提及 tzolov/spring-ai-mcp 为实验性的 Spring AI MCP。
C#: modelcontextprotocol/csharp-sdk (官方, 与 Microsoft 共同维护36)。40 提及 MCPSharp SDK。
Ruby: modelcontextprotocol/ruby-sdk (官方, 与 Shopify 合作37)。
Swift: modelcontextprotocol/swift-sdk (官方, 与 @loopwork-ai 合作37)。
Kotlin: 在 29 中提及可用，但 37 未像其他语言一样置顶独立的 SDK 仓库。可能包含在 Java SDK 中或是较新的补充。
背景与重要性： 提供多种流行语言的官方 SDK 显著降低了开发人员的入门门槛，并促进了更广泛的采用。
协作式 SDK 开发： 多个 SDK（Java 与 Spring AI、C# 与 Microsoft、Ruby 与 Shopify、Swift 与 loopwork-ai 35）是与其他组织合作维护的。这种协作模式是 SDK 健康、质量和实际适用性的强烈积极指标，因为它汇集了多样化的专业知识，并确保 SDK 满足不同平台和社区的需求。协作带来了更多的开发人员、测试人员和领域专家，可能会提高其质量和功能集。例如，与 Spring AI 合作的 Java SDK 意味着它很可能无缝集成到 Spring 生态系统中。与 Microsoft 合作的 C# SDK 表明与.NET 平台和 Azure 的良好结合。当特定语言生态系统中的主要参与者共同维护 SDK 时，这标志着认可，并鼓励这些生态系统中的开发人员采用 MCP。协作项目通常比由单个实体维护的项目更具可持续性，因为维护负担是分担的。因此，MCP SDK 的协作开发模式是一项战略优势。它促进了更高质量的 SDK、更好的生态系统集成和更广泛的采用，使 MCP 对不同语言偏好的开发人员更具吸引力和稳健性。
表2：官方 MCP SDK 概览
语言
SDK GitHub 仓库 (链接)
主要特性/备注 (例如：“完整 MCP 规范”，“Stdio/SSE 支持”，“与 X 共同维护”)
核心类 (客户端/服务器)
Python
modelcontextprotocol/python-sdk
完整 MCP 规范, Stdio/HTTP 支持, FastMCP 简化开发
McpClient, McpServer / FastMCP
TypeScript
modelcontextprotocol/typescript-sdk
完整 MCP 规范, Stdio/SSE/StreamableHTTP 支持, Zod 模式验证
Client, McpServer
Java
modelcontextprotocol/java-sdk
与 Spring AI 共同维护, 支持 Stdio/HTTP SSE, 同步/异步
McpClient, McpServer
C#
modelcontextprotocol/csharp-sdk
与 Microsoft 共同维护, NuGet 包, Stdio 支持
IMcpClient (McpClientFactory), McpServer
Ruby
modelcontextprotocol/ruby-sdk
与 Shopify 共同维护
(查阅 SDK 文档)
Swift
modelcontextprotocol/swift-sdk
与 @loopwork-ai 共同维护
(查阅 SDK 文档)
Kotlin
(可能包含在 Java SDK 中或查阅官方文档)
(查阅 SDK 文档)
(查阅 SDK 文档)

3.3. 安装 SDK（以 Python 或 TypeScript 为例）
Python 数据点： uv pip install modelcontextprotocol-sdk (源自使用 uv 的快速入门)。
TypeScript 数据点： npm install @modelcontextprotocol/sdk 31 或 yarn add @modelcontextprotocol/sdk。
背景与重要性： 为开发人员提供将 SDK 引入项目的实际步骤。
SDK 命名一致性： Python 的包名（可能为 modelcontextprotocol-sdk，基于通用 Python 打包约定，尽管片段中未明确说明，Java 的 artifactId 为 mcp 35）和 TypeScript 的 @modelcontextprotocol/sdk 31 显示了在命名一致性方面的努力，这有利于可发现性。npm 包的 @modelcontextprotocol 作用域是常见的最佳实践。一致的命名有助于开发人员在 PyPI 和 npm 等包存储库中找到 SDK。npm 上的 @modelcontextprotocol 作用域清楚地将其标识为与协议相关的官方包。因此，像包命名这样的小细节有助于构建一个专业且对开发人员友好的生态系统。
3.4. 连接到参考 MCP 服务器（例如文件系统服务器）
数据点： OpenAI Agents SDK 示例使用 MCPServerStdio 连接官方 MCP 文件系统服务器：npx -y @modelcontextprotocol/server-filesystem <samples_dir> 3。通用服务器列表位于 modelcontextprotocol/servers GitHub 12。
背景与重要性： 展示了一个实际的初始步骤：使用 SDK 连接到现有的、功能正常的服务器。这有助于验证设置并理解基本的客户端-服务器交互。文件系统服务器因其明确的实用性而成为一个良好的起点。
使用 npx 引导： 使用 npx -y @modelcontextprotocol/server-filesystem 3 允许开发人员在不全局安装的情况下运行参考服务器。这是一种方便的、现代的 JavaScript 生态系统实践，降低了试用服务器的门槛。npx 执行 npm 包二进制文件。-y 标志（如果支持，或者如果它意味着对提示自动回答“是”）或者仅仅是 npx 本身通常意味着如果本地找不到包，它会临时下载该包，然后运行它。这避免了污染全局命名空间或需要单独的安装步骤。开发人员只需一条命令即可快速启动参考服务器。npx 通常使用最新版本，除非另有指定，这对于针对最新的服务器代码进行测试非常有用。因此，使用 npx 运行参考服务器表明了对开发人员体验的关注，使得开始测试和集成变得非常容易。
3.5. 用于测试的 MCP Inspector 简介
数据点： MCP Inspector 是一款用于直接服务器测试的交互式调试工具 7。可以通过 npx @modelcontextprotocol/inspector 安装 44。用于测试和调试流程、监控工具注册、使用自定义输入测试工具 44。
背景与重要性： 专用的测试工具对于 MCP 开发非常有价值。它允许开发人员在将服务器集成到更大的主机应用程序之前对其进行隔离测试，并检查消息和能力。
Inspector 作为中立测试平台： MCP Inspector 充当参考 MCP 客户端实现。这一点至关重要，因为它允许服务器开发人员验证其服务器是否根据协议正确运行，而与任何特定主机应用程序的客户端逻辑无关。它有助于区分服务器错误和客户端集成问题。在开发 MCP 服务器时，您希望在尝试将其与像 Claude Desktop 或自定义 AI 代理这样的复杂主机集成之前，确保其符合协议。Inspector 实际上充当了一个已知良好的客户端。如果您的服务器与 Inspector 一起工作，那么它更有可能与其他兼容的客户端一起工作。如果它不与 Inspector 一起工作，问题可能出在您的服务器上。它允许您“监控您的流程如何被 MCP 注册为工具，以及使用自定义输入值测试工具” 44。这是对服务器能力的直接、交互式调试。因此，MCP Inspector 不仅仅是一个“测试器”；它是一个重要的开发工具，为服务器开发提供标准化的客户端环境，从而实现有针对性的调试和协议合规性检查。这加速了服务器开发并提高了可靠性。社区中也存在其他工具，如 mcp-reporter 46 和 MCP Tools 47，它们提供了不同的实用功能。
4. 使用 MCP：客户端集成与用法
一旦环境设置完毕并且对 MCP 的基本概念有了理解，下一步就是将 MCP 客户端集成到应用程序中，并开始利用 MCP 服务器提供的能力。
4.1. 连接 MCP 服务器 (SDK 示例)
OpenAI Agents SDK / Python 数据点：
MCPServerStdio(params={"command": "npx", "args": [...]}) 3。
用于远程服务器的 MCPServerSse 和 MCPServerStreamableHttp 类 3。
TypeScript SDK 数据点：
StdioClientTransport({ command: "node", args: ["server.js"] }) 然后 client.connect(transport) 27。
StreamableHTTPClientTransport 以及回退到已弃用的 SSEClientTransport 27。
C# SDK 数据点：
StdioClientTransport 然后 McpClientFactory.CreateAsync(clientTransport) 36。
背景与重要性： 展示了不同的 SDK 如何提供抽象来连接到使用各种传输方式的服务器。客户端需要知道服务器类型（stdio、HTTP）及其连接详细信息（命令、URL）。
SDK 中的传输抽象： SDK 旨在抽象传输管理的低级细节。例如，OpenAI SDK 中的 MCPServerStdio 处理子进程管理和 I/O 管道通信。这使得开发人员能够专注于 MCP 逻辑（列出工具、调用工具），而不是传输的复杂性。设置 stdio 通信（启动子进程、管理其 stdin/stdout/stderr、处理进程生命周期）或管理带有 SSE 的 HTTP 连接（处理事件、重新连接、请求/响应映射）可能很复杂。这些 SDK 类封装了这种复杂性。开发人员使用配置（命令字符串、URL）实例化适当的传输/服务器类，SDK 处理其余部分。连接后，用于与 MCP 交互的更高级别的客户端 API（例如 list_tools()、call_tool()）通常与底层传输无关。因此，MCP SDK 中的传输抽象是提高开发人员生产力的关键特性，允许他们在更高的概念级别上使用 MCP，而无需陷入进程间或网络通信的具体细节。
4.2. 发现能力：列出工具、资源和提示
数据点： Agents SDK 每次运行 Agent 时都会在 MCP 服务器上调用 list_tools() 3。TypeScript 客户端：client.listPrompts()、client.listResources() 27。C# 客户端：client.ListToolsAsync() 36。MCP 规范：tools/list 端点 28。
背景与重要性： 在 LLM 或应用程序可以使用服务器的能力之前，它必须首先发现哪些能力可用。这种动态发现是 MCP 的核心功能。
能力的动态性： 能力可以在运行时更改。服务器可以通知客户端更改（例如，notifications/tools/list_changed 28）。这意味着客户端不应假定能力的静态列表，可能需要刷新或侦听更新，特别是对于长连接。缓存（见 4.6 节）是一种优化，但需要失效策略。客户端调用 list_tools() 3。文献 28 提到“客户端可以随时列出可用工具”和“服务器可以使用 notifications/tools/list_changed 通知客户端工具何时更改。” 这种动态性对客户端设计有何影响？如果客户端严重依赖缓存工具列表（如 3 中为提高性能所建议的那样），但服务器的工具发生更改，则客户端可能会使用过时的信息进行操作，从而导致错误或错失机会。健壮的客户端可能需要：定期重新列出工具；或者，更有效地，支持处理来自服务器的 list_changed 通知以更新其内部状态/缓存。处理动态能力更新会增加客户端的复杂性，但使整个系统对服务器端更改更具灵活性和响应性。因此，虽然列出工具看起来很简单，但动态更改的可能性意味着客户端开发人员必须考虑保持其服务器能力视图最新的策略，在性能（缓存）和准确性（处理更新）之间取得平衡。list_changed 通知是实现此目的的关键机制。
4.3. 调用工具并处理响应
数据点： LLM 调用工具，SDK 在服务器上调用 call_tool() 3。TypeScript 客户端：client.callTool({ name: "tool-name", arguments: {... } }) 27。C# 客户端：client.CallToolAsync("tool-name", params, cancellationToken) 36。MCP 规范：tools/call 端点 28。响应包含 result 或 error 15。
背景与重要性： 这是实际操作发生的地方。客户端（通常由 LLM 指导）请求服务器执行操作。正确的参数传递和响应/错误处理至关重要。
LLM 驱动的工具调用： 在许多 MCP 用例中，LLM 根据用户的查询和发现的工具模式来决定调用哪个工具以及使用什么参数 3。然后，MCP 客户端充当执行桥梁。这突出了 LLM 推理与 MCP 工具执行之间的紧密耦合。“当 LLM 从 MCP 服务器调用工具时，SDK 会在该服务器上调用 call_tool()” 3。“工具被设计为模型控制的” 28。导致 call_tool() 的典型流程是什么？用户向主机应用程序提供查询/任务。主机（可能及其 LLM）查阅可用工具（来自 list_tools()）。LLM 理解用户的意图和工具的描述/模式，决定工具是否合适。如果是，LLM 会生成工具名称和该工具的参数。然后，主机的 MCP 客户端获取 LLM 的这种结构化输出，并向 MCP 服务器发出实际的 call_tool() 请求。工具的 inputSchema 28 在此至关重要。LLM 使用此模式来理解工具期望哪些参数以及采用何种格式，从而使其能够生成有效的参数。因此，MCP 中的工具调用通常不是直接的用户操作，而是 LLM 介导的操作。工具描述和模式的质量直接影响 LLM 有效使用工具的能力。MCP 客户端促进了这种 LLM 驱动的交互。
4.4. 读取和利用资源
数据点： TypeScript 客户端：client.readResource("uri") 27。资源是类文件数据 29。
背景与重要性： 客户端需要访问服务器提供的上下文数据，以通知 LLM 或应用程序逻辑。
基于 URI 的资源寻址： 资源通常通过 URI 进行标识和访问（例如，config://app、users://{userId}/profile 31，file:///example.txt 31）。这种基于 URI 的方案允许对不同资源类型进行灵活和标准化的寻址，从本地文件到服务器定义的抽象数据。资源示例使用诸如 config://app 或 users://{userId}/profile 31 以及 file:///example.txt 31 之类的 URI。readResource 接受一个 URI。为什么对资源使用 URI？URI 提供了一种广为人知、标准化的资源识别方式，无论其底层性质或位置如何。服务器可以定义自定义 URI 方案（例如 users://、config://）来表示其特定的数据组织。标准方案（如 file://）可用于本地文件。URI 可以嵌入参数（如 users://{userId}/profile 中的 userId），如 31 中的 ResourceTemplate 所示，从而允许动态检索资源。因此，使用 URI 对 MCP 资源进行寻址提供了一种强大而灵活的机制，该机制符合 Web 标准，并允许服务器以结构化方式公开各种数据。然后，客户端可以使用这些 URI 来请求特定的上下文片段。
4.5. 使用提示 (Prompts)
数据点： TypeScript 客户端：client.getPrompt("prompt-name", { args }) 27。
背景与重要性： 客户端可以从服务器获取预定义的提示模板，填充参数，然后将这些提示与 LLM 一起使用。
客户端提示实例化： 客户端从服务器获取提示模板，然后使用特定参数对其进行实例化 27。服务器提供结构和指导，但客户端（或主机）负责最终的提示组装并将其发送给 LLM。客户端调用 getPrompt("prompt-name", { arg1: "value" }) 31。服务器使用参数模式定义提示，例如 server.prompt("review-code", { code: z.string() }, ({ code }) => ({ messages: [...] })) 31。在提示生命周期中，谁负责什么？服务器定义提示的名称、其预期的参数（模式）和模板结构（例如，带有参数占位符的消息数组）。客户端按名称请求特定提示并提供参数的实际值。然后，服务器可能会返回已插入参数的消息结构，或者客户端执行此插值。片段 31 显示了服务器端处理函数接收参数并返回消息结构。然后，客户端/主机获取这个完全形成的提示（消息数组）并将其发送给其 LLM。因此，MCP 提示涉及协作：服务器提供模板和逻辑，客户端提供用于填充它的特定数据。这允许可重用的、服务器引导的交互，同时仍然让客户端控制发送给 LLM 的提示中使用的最终数据。
4.6. 缓存工具列表以提高性能
数据点： 每次代理运行时调用 list_tools() 可能会造成延迟。可以通过向 MCPServerStdio、MCPServerSse、MCPServerStreamableHttp 传递 cache_tools_list=True 来实现缓存 3。仅当工具列表不会更改时才执行此操作。可以使用 invalidate_tools_cache() 使缓存失效 3。
背景与重要性： 针对频繁运行的代理或应用程序的性能优化。
权衡：性能与动态性： 缓存工具列表 3 引入了一个经典的权衡。它通过避免重复的 list_tools() 调用（尤其对于远程服务器）来提高性能。但是，如果服务器的工具可以动态更改（根据 28），则过时的缓存可能导致错误，或者 LLM 可能不知道新的/更新的工具。invalidate_tools_cache() 方法和 notifications/tools/list_changed 28 是管理此问题的方法，但需要仔细实现。文献 3 建议缓存 list_tools() 结果以提高性能，但警告仅在列表是静态的情况下才这样做。文献 28 指出可以在运行时添加/删除工具，并且服务器可以发送 notifications/tools/list_changed。客户端应如何协调这些？如果已知服务器具有固定的工具集，则 cache_tools_list=True 是一种安全有效的优化。如果服务器的工具可以更改，则没有失效策略的积极缓存是有风险的。失效策略包括：手动失效：应用程序在怀疑或知道工具可能已更改时调用 invalidate_tools_cache()。这很简单，但可能不及时。通知驱动的失效：更高级的客户端可以侦听来自服务器的 notifications/tools/list_changed，并自动调用 invalidate_tools_cache() 或重新获取工具列表。这响应更快，但增加了复杂性。生存时间 (TTL) 缓存：缓存一小段时间然后刷新。因此，cache_tools_list=True 选项是一个有用的性能特性，但开发人员必须了解他们所连接的 MCP 服务器的性质。对于动态服务器，需要更复杂的缓存策略，包括 invalidate_tools_cache() 并可能处理 list_changed 通知，以确保客户端和 LLM 对可用工具具有准确的视图。
4.7. 将 MCP 与 AI 代理集成（例如 OpenAI Agents SDK 示例）
数据点： MCP 服务器可以添加到 OpenAI Agents SDK 中的代理。SDK 调用 list_tools() 和 call_tool() 3。agent = Agent(mcp_servers=[server1, server2]) 3。Azure OpenAI 与 Chainlit 的示例 5。
背景与重要性： 这是一个关键用例——通过 MCP 为 AI 代理赋能外部能力。
代理作为 MCP 主机： 当 AI 代理框架（如 OpenAI Agents SDK）集成 MCP 服务器时，代理框架本身（或使用它的应用程序）充当 MCP 主机。它协调 LLM、用户和连接到已配置 MCP 服务器的各种 MCP 客户端之间的交互。OpenAI Agents SDK 允许将 mcp_servers 添加到 Agent 3。然后 SDK 处理调用 list_tools() 和 call_tool()。这如何适应 MCP 主机/客户端/服务器模型？定义和运行 Agent 的应用程序代码（例如，使用 OpenAI Agents SDK）有效地充当 MCP 主机。SDK 本身可能实例化和管理与每个注册的 mcp_server 通信所需的 MCP 客户端组件。代理核心的 LLM 负责根据用户的任务和通过 list_tools() 提供的工具来决定何时使用来自 MCP 服务器的工具。因此，将 MCP 集成到代理框架中涉及框架承担 MCP 主机的角色，从而为构建代理的最终开发人员抽象了大部分直接的 MCP 客户端管理。这使得将各种工具插入 AI 代理变得更加容易。
4.8. 常见的客户端错误处理和最佳实践
数据点： 快速入门指南：将工具调用包装在 try-catch 中，提供有意义的错误消息，处理连接问题，使用 AsyncExitStack 进行清理，安全存储 API 密钥，验证服务器响应，谨慎对待工具权限 41。响应时间：第一个响应可能需要长达 30 秒 41。故障排除：FileNotFoundError（检查服务器路径），连接被拒绝（服务器正在运行吗？），工具执行失败（环境变量？），超时 41。
背景与重要性： 强大的错误处理和遵守最佳实践对于构建可靠的 MCP 客户端应用程序至关重要。
客户端的安全责任： 尽管服务器提供能力，但客户端/主机对安全负有重大责任，特别是在用户同意、验证服务器响应（在某种程度上）和管理工具权限方面 13。这是因为客户端是与用户和 LLM 交互的实体。客户端最佳实践包括“验证服务器响应”和“谨慎对待工具权限” 41。文献 13 强调用户同意和控制权在于主机。为什么如果服务器提供工具，客户端仍负有这些安全责任？客户端/主机是用户与之交互的内容。它负责清楚地说明工具将执行的操作，并在执行前获得明确的用户同意 13。客户端不应盲目信任来自任何 MCP 服务器的所有数据，尤其是在服务器不受信任或是第三方服务器的情况下。某种程度的验证（例如，响应是否符合预期模式？）可能是有益的。主机应用程序可能有其自己的权限模型，用于确定哪些用户可以访问哪些 MCP 服务器或工具，即使服务器本身愿意执行操作。主机控制 LLM。它负责确保来自 MCP 服务器的信息在提示中安全使用，并且用于工具调用的 LLM 输出是适当的。因此，MCP 安全是共同的责任。虽然服务器必须安全构建，但客户端/主机充当关键的网守，管理用户同意、控制 LLM 交互，并可能应用其自身的验证和权限层。这对于构建值得信赖的 AI 应用程序至关重要。
5. 使用 MCP 进行开发：构建您自己的服务器
构建自定义 MCP 服务器可以释放特定数据源或工具的全部潜力，使其能够与任何兼容 MCP 的 AI 应用程序无缝集成。
5.1. 为服务器开发选择合适的 SDK
数据点： 提供 Python、TypeScript、Java、C# 等 SDK（参见 3.2 节）。选择取决于语言偏好、现有代码库和生态系统。
背景与重要性： 选择与开发团队技能和项目技术栈相符的 SDK 对于高效的服务器开发至关重要。
SDK 功能对等性和成熟度： 虽然存在多个 SDK，但它们的功能对等性、成熟度和社区支持可能有所不同。TypeScript 和 Python SDK 似乎在示例和核心文档中被大量引用（例如，TypeScript 为 31；Python 为 29；12 提及参考服务器使用 TS/Python SDK）。开发人员应调查所选语言 SDK 的当前状态，特别是对于更高级的 MCP 功能。TypeScript（协议模式的事实来源 15）和 Python（在 AI/ML 中流行）的突出地位表明这些 SDK 可能是功能最完整或更新最快的。合作开发的 SDK（Java/Spring、C#/.NET 35）很可能与其各自的生态系统很好地集成，但可能优先考虑与这些生态系统相关的功能。可能存在社区复刻或替代 SDK（例如，C# 的 MCPSharp 40）。开发人员需要评估这些是否提供优势，或者坚持使用 modelcontextprotocol 组织的的官方版本是否更安全。因此，在选择 SDK 时，除了语言偏好之外，开发人员还应考虑其成熟度、关于 MCP 规范的完整性（特别是对于像 Streamable HTTP 或高级采样这样的新功能）、文档质量和社区活动。根据现有材料，TypeScript 和 Python SDK 似乎是主要的强力选择。
5.2. 服务器快速入门（以 Python 或 TypeScript SDK 为例的说明性示例）
Python 数据点： FastMCP 类使用类型提示和文档字符串自动生成工具定义 29。示例天气服务器包含 get-alerts 和 get-forecast 工具 29。
TypeScript 数据点： McpServer 类。server.tool()、server.resource()、server.prompt() 方法与 Zod 一起用于模式验证 27。Echo 服务器和 SQLite 浏览器示例 27。
背景与重要性： 创建基本功能服务器的分步指南，演示定义工具、资源和提示等核心概念。
声明式工具定义： Python (FastMCP 与类型提示/文档字符串 29) 和 TypeScript (用于 server.tool 的 Zod 模式 31) SDK 都提倡以声明方式定义工具模式。这简化了开发，减少了样板代码，并利用语言特性进行验证和内省。Python 的 FastMCP 使用类型提示和文档字符串 29。TypeScript 示例使用 Zod 定义工具的输入模式 31。这些方法有什么好处？开发人员通常可以使用语言结构更自然地定义模式，而不是手动制作复杂的 JSON 模式对象，从而减少了样板代码。类型提示 (Python) 和 Zod 模式 (TypeScript) 提供静态分析和运行时验证，及早发现错误。SDK 可以内省这些定义以自动生成 MCP 协议所需的 inputSchema。这使得定义工具更清晰、更不容易出错，并且更符合这些语言中的现代开发实践。因此，SDK 旨在通过利用特定语言的惯用方法来声明数据结构和验证规则，从而尽可能直接地定义工具，然后将其转换为符合 MCP 的模式。这是一个显著的开发体验 (DX) 改进。
5.3. 实现传输层
Python 数据点： 在简单的服务器示例中，Stdio 通常是隐式的。
TypeScript 数据点： 提供了 StdioServerTransport 和 SSEServerTransport (使用 Express 进行 HTTP) 的示例 27。
背景与重要性： 服务器必须选择并实现传输层才能与客户端通信。Stdio 通常用于本地服务器，而基于 HTTP 的传输则用于远程服务器。
服务器端传输复杂性： 虽然 SDK 简化了传输实现，但设置基于 HTTP 的传输（例如，使用 Express 的 TypeScript 31）仍然需要 Web 服务器知识（路由、请求处理、SSE 生命周期）。Stdio 更简单，因为 SDK 通常处理 I/O 循环。TypeScript SDK 示例显示了使用 StdioServerTransport（简单连接）与 SSEServerTransport，后者需要设置 Express 应用程序、用于 SSE 和 POST 消息的路由 31。对于服务器开发人员来说，不同的复杂性是什么？对于 stdio，SDK 通常管理从 stdin 读取和向 stdout 写入。服务器开发人员专注于 MCP 消息处理程序。对于 HTTP/SSE，开发人员需要：设置 HTTP 服务器（例如 Express）。定义用于 MCP 通信的路由（例如，SSE 端点、用于客户端消息的 POST 端点，如 31 所示，或用于 Streamable HTTP 的单个 /mcp 端点 27）。将 MCP SDK 的传输处理程序与 HTTP 服务器的请求/响应对象集成。管理特定于 HTTP 的问题，如标头、状态代码、潜在的 CORS。因此，即使有 SDK 支持，与 stdio 服务器相比，实现基于 HTTP 的 MCP 服务器也涉及更多的基础架构设置和 Web 开发知识。SDK 提供了 MCP 逻辑，但开发人员负责 Web 服务器外壳。
5.4. 处理客户端请求和服务器响应
数据点： 服务器实现工具调用、资源读取、提示请求的处理程序。这些处理程序接收参数，执行逻辑，并返回结构化的结果或错误（示例见 29）。
背景与重要性： 这是 MCP 服务器的核心逻辑——根据定义的能力满足客户端请求。
服务器中的异步操作： 许多服务器操作（调用外部 API、文件 I/O、数据库查询）本质上是异步的。MCP 服务器 SDK 和示例大量使用 async/await（例如，Python async def get_alerts 29，TypeScript 工具处理程序 31）。正确处理异步操作对于服务器响应能力和避免阻塞至关重要。服务器端工具实现示例通常使用 async 函数 29。为什么异步编程在 MCP 服务器开发中如此普遍？MCP 服务器执行的许多任务（获取 Web 内容、查询数据库、读取文件、调用其他 API）都是 I/O 密集型的。这些任务的同步执行会阻塞服务器，使其无法处理其他请求。异步编程允许服务器并发处理多个客户端请求，从而提高响应能力和吞吐量。SDK 本身可能在设计时就考虑了异步操作，以支持这些常见用例（例如，TypeScript 中的 McpServer.tool 处理程序通常返回一个 Promise）。因此，有效的 MCP 服务器开发，特别是对于与外部系统交互的工具和资源，需要充分理解所选语言中的异步编程模式，以确保服务器保持高性能和响应迅速。
5.5. 管理服务器状态和生命周期
数据点： McpServer 构造函数接受名称、版本 31。连接生命周期包括初始化、交换、终止 16。动态服务器可以在连接后修改工具/提示/资源 27。
背景与重要性： 服务器可能需要管理与其能力或连接的客户端相关的内部状态。正确的生命周期管理可确保干净的启动和关闭。
动态服务器能力管理： 服务器在连接后动态添加、更新或删除工具/提示/资源的能力（27，来自 TypeScript SDK 高级用法）是一项复杂的功能。这意味着服务器需要管理其可用能力的状态，并正确地向所有连接的客户端发出 listChanged 通知。这对于自适应服务器来说功能强大，但增加了状态管理的复杂性。TypeScript SDK 支持在连接后动态修改服务器能力，并触发 listChanged 通知 27。这对服务器端状态管理有何影响？服务器必须维护其当前工具、资源和提示的内部注册表或状态。当能力被添加、更新或删除时（例如，由于管理员操作或底层系统的更改），服务器逻辑必须：更新此内部注册表；构建并向所有相关的已连接客户端发送适当的 listChanged 通知。如果连接了多个客户端，则必须正确处理这些更新和通知，可能需要考虑对能力注册表的并发访问。这对于以下场景很有用：根据用户身份验证/权限启用/禁用功能；根据外部系统的状态使工具可用/不可用；逐步推出新的服务器功能。因此，动态能力管理是一项高级服务器功能，需要仔细的状态管理和强大的通知逻辑。它允许高度自适应和上下文感知的 MCP 服务器，但与具有静态能力的服务器相比，增加了实现复杂性。
5.6. 服务器开发最佳实践（模块化、错误处理、日志记录）
数据点： 保持工具的专注性和原子性 28。实现正确的错误处理和验证 28。记录预期的返回值结构 28。对长时间运行的操作使用进度报告 16。实现超时 16。记录工具使用情况 28。结构化日志记录、错误处理、性能跟踪 43。
背景与重要性： 将通用的软件工程最佳实践应用于 MCP 服务器开发，可以产生更健壮、可维护和可调试的服务器。
适用情况下工具的幂等性： 虽然在这些片段中没有明确说明这是 MCP 的通用最佳实践，但对于会产生副作用的工具，如果可能，将其设计为幂等的是一个有价值的原则。如果客户端由于网络错误而重试 call_tool 请求，幂等工具可确保多次应用该操作与应用一次具有相同的效果。这简化了客户端的错误恢复。最佳实践包括错误处理、超时、进度报告 16。网络问题或超时可能导致客户端重试请求。如果修改状态的工具调用被重试会发生什么？与资源不同，工具可能会产生副作用 31。如果像 create_user 这样的工具由于重试而被调用两次，它可能会创建两个用户或在第二次尝试时失败。幂等的 create_user 工具，如果使用相同的参数第二次调用，将识别用户已存在并返回成功或特定的“已存在”状态，而不会创建重复项。如果工具是幂等的，客户端重试逻辑会变得简单得多。客户端不必太担心操作是否在错误发生前已部分执行。因此，虽然 MCP 本身并未强制要求幂等性，但服务器开发人员应在操作语义允许的情况下，强烈考虑将具有副作用的工具设计为幂等的。这显著提高了整个客户端-服务器交互的健壮性，尤其是在面对瞬时错误时。这是分布式系统设计中常见的最佳实践，在此同样适用。
6. 高级主题与最佳实践
随着对 MCP 的理解加深，开发人员可以探索更高级的主题以构建更复杂、安全和高效的应用程序。
6.1. 安全深度探讨
6.1.1. 用户同意与控制
数据点： 用户必须明确同意数据访问和操作 13。主机在暴露用户数据或调用工具之前必须获得同意 13。需要清晰的用户界面进行审查/授权 13。
背景与重要性： 这是 MCP 安全的基石，确保用户拥有决定权。
精细化同意的挑战： 实现有意义且精细化的用户同意可能具有挑战性。笼统的“允许此服务器”同意不如针对每个工具或每种资源类型的同意，或针对具有特定参数的特定操作的同意安全。主机需要在可用性与安全精细度之间取得平衡。“用户必须明确同意” 13。“用于审查和授权活动的清晰用户界面” 13。在实践中，“明确同意”对于复杂交互意味着什么？对整个服务器的一次性同意很简单，但可能会授予过多权限。对每个工具调用及其所有参数都要求同意可能会让用户不知所措。主机可能会根据工具或资源的感知风险实施不同级别的同意。只读资源可能需要不如删除数据或进行具有副作用的 API 调用的工具那么严格的同意。工具注释 28 可以帮助主机 UI 向用户解释工具的功能，从而帮助用户做出知情同意。可能不仅需要对工具本身表示同意，还需要对 LLM 提议发送给该工具的数据表示同意。因此，在 MCP 主机中有效地实施用户同意需要仔细的 UX 设计和对精细度的深思熟虑的方法。这不仅仅是一个复选框；它关乎向用户提供可理解的信息以及对 AI 如何与其数据和外部系统交互的有意义的控制。这是主机应用程序质量将显著不同的一个领域。
6.1.2. 数据隐私注意事项
数据点： 未经用户同意，主机不得将资源数据传输到其他地方 13。用户数据受访问控制保护 13。
背景与重要性： 保护通过 MCP 共享的用户数据至关重要。
数据最小化原则： 虽然没有明确说明，但数据最小化原则（仅向服务器提供必要的上下文信息，14：“服务器不应能够读取整个对话……服务器仅接收必要的上下文信息”）是 MCP 设计中增强隐私的关键方面。主机在执行此原则方面发挥着至关重要的作用。“服务器仅接收必要的上下文信息。完整的对话历史记录保留在主机中。” 14。主机在公开用户数据之前必须获得同意 13。这与数据隐私最佳实践有何关联？这符合数据最小化原则——仅收集和处理特定目的所必需的数据。主机充当过滤器或网守，决定对话的哪些部分或哪些用户数据是相关的，应传递给特定 MCP 服务器的工具或资源。通过限制暴露给每个服务器的数据，可以减少受感染服务器或行为不当工具的潜在影响。因此，MCP 的架构通过主机对上下文共享的控制，从设计上鼓励数据最小化。这是一项显著的隐私增强功能，但其有效性取决于主机应用程序中智能且谨慎的实现。
6.1.3. 工具安全与执行沙箱
数据点： 工具代表任意代码执行，需谨慎对待 13。除非服务器受信任，否则工具描述/注释不可信 13。主机在调用工具前必须获得同意 13。
背景与重要性： 防止恶意或有缺陷的工具造成损害。
主机端沙箱/中介： 虽然 MCP 服务器执行工具，但主机应用程序可能会实现其自身的沙箱或中介层，尤其是在处理不受信任的服务器或具有高风险能力的工具时。这可能涉及超出模式检查范围的参数验证，甚至在主机控制的沙箱环境中运行某些类型的“本地”工具（例如代码解释器）。“工具代表任意代码执行，必须谨慎对待” 13。谁负责确保这种谨慎，以及如何确保？服务器开发人员有责任确保其工具安全地执行其声明的功能，并正确处理输入和错误。他们应该遵循安全编码实践，并对其工具可能与之交互的任何外部系统进行身份验证和授权。主机（客户端应用程序）通过获取用户对工具执行的明确同意来扮演关键角色 13。主机还可能决定是否信任特定服务器或工具，并可能限制其能力或在沙箱环境中运行它们（如果适用）。例如，像 Zed 这样的 IDE 18 或其他主机应用程序可能会对从 MCP 服务器接收的代码执行请求实施额外的安全层。因此，工具安全是服务器开发人员和主机应用程序开发人员的共同责任。服务器必须安全地实现工具，而主机必须通过用户同意、信任管理和潜在的沙箱机制来控制这些工具的执行。
6.1.4. 传输安全 (TLS, Origin 验证等)
数据点： 远程连接使用 TLS 16。验证连接来源 (Origin header) 16。STDIO 安全依赖操作系统 24。
背景与重要性： 确保数据在客户端和服务器之间传输过程中的机密性和完整性。
STDIO 与 HTTP 传输的安全模型差异： STDIO 传输的安全性主要依赖于操作系统的进程隔离和用户权限模型 24。当主机应用程序启动一个 STDIO 服务器（通常作为子进程）时，它们之间的通信发生在同一台机器的受控环境中。相比之下，HTTP 传输（如 Streamable HTTP 或 SSE）通过网络进行通信，因此面临着典型的网络安全威胁。因此，它们必须采用如 TLS/SSL 加密、严格的来源验证（例如，检查 Origin HTTP 头部以防止 DNS 重新绑定攻击 22）和强大的身份验证/授权机制（如 MCP 的 OAuth 2.1 框架 15）。这种差异意味着为本地 STDIO 服务器编写的安全假设可能不适用于远程 HTTP 服务器，后者需要更全面的网络安全措施。
6.1.5. 身份验证与授权 (OAuth 2.1 流程)
数据点： MCP 为 HTTP 传输提供授权框架，基于 OAuth 2.1 及相关 RFC 15。STDIO 应从环境获取凭据 15。禁止令牌传递 25。
背景与重要性： 验证客户端身份并确保其有权访问特定服务器功能。
禁止令牌传递的重要性： MCP 授权规范明确禁止“令牌传递” 25。这意味着 MCP 服务器不应简单地将其从客户端收到的访问令牌直接转发给下游的受保护资源（例如第三方 API）。相反，MCP 服务器本身应作为 OAuth 客户端与下游资源进行交互，使用其自身的凭据或通过令牌交换机制获取针对该下游资源的特定令牌。这一规则至关重要，原因如下：它防止了“混淆代理人”问题，即下游 API 可能会错误地信任该令牌，好像它直接来自受信任的 MCP 服务器，或者假设该令牌具有 MCP 服务器的权限。它确保了 MCP 服务器能够对其代表客户端对下游资源执行的操作负责，并能够实施自己的访问控制、审计和速率限制策略。它维护了清晰的信任边界；下游资源仅需信任特定的 MCP 服务器，而不是潜在的众多 MCP 客户端。如果允许令牌传递，恶意客户端可能会利用拥有访问 MCP 服务器权限的令牌来访问该 MCP 服务器有权访问但客户端本身无权访问的下游资源。因此，虽然这给需要与受 OAuth 保护的外部 API 交互的 MCP 代理服务器增加了一些实现复杂性，但禁止令牌传递是维护整体系统安全性和问责制的关键设计决策。
6.1.6. 输入验证与清理
数据点： 彻底验证输入，使用类型安全的模式 16。清理输入，检查消息大小限制，验证 JSON-RPC 格式 16。清理文件路径和系统命令，验证 URL 和外部标识符 28。
背景与重要性： 防止注入攻击、拒绝服务和其他基于恶意输入的漏洞。
服务器和客户端的双重责任： 输入验证不仅仅是服务器端的责任。虽然服务器必须严格验证所有来自客户端的输入（例如，工具参数、资源请求中的 URI），以防止诸如 SQL 注入、命令注入或路径遍历等攻击 16，但客户端（主机应用程序）在将用户输入或 LLM 生成的内容传递给 MCP 服务器之前，也应执行其自身的验证和清理。例如，如果 LLM 生成用于工具调用的参数，主机应在将其发送到服务器之前验证这些参数是否符合工具的输入模式，并且不包含明显的恶意内容。这种双重检查方法，即服务器执行权威验证，客户端执行初步筛选，有助于构建更强大的多层防御系统。
6.1.7. 速率限制与资源保护
数据点： 实施访问控制，验证资源路径，监控资源使用，对请求进行速率限制 16。
背景与重要性： 防止滥用、确保服务公平性并保护后端系统免受过载。
上下文感知的速率限制： 除了通用的请求速率限制（例如，每个客户端每秒的请求数），MCP 服务器还可以考虑实现更细致的、上下文感知的速率限制。例如，某些工具可能比其他工具消耗更多的计算资源或访问更敏感的数据。服务器可以根据所调用的工具、请求的资源或经过身份验证的用户的权限级别来应用不同的速率限制策略。此外，如果服务器支持动态能力更新，当检测到潜在的滥用或系统过载时，它甚至可以暂时禁用某些高成本工具或限制对某些资源的访问，并通过 listChanged 通知告知客户端。这种动态的、基于风险的资源保护方法可以提高系统的整体弹性和安全性。
6.2. 性能与可伸缩性考量
数据点： 缓存工具列表 3。对长时间操作使用进度报告 16。实现超时 16。监控性能 16。BytePlus 的文章讨论了 MCP HTTP/SSE 的低延迟、高可扩展性和最小开销，并与 Stdio MCP 和 WebSockets 进行了比较 48。还提到了连接管理、安全协议和性能优化策略（最小化有效负载、高效序列化、智能缓存）48。另一篇文章提到了 MCP 框架可以将上下文传输延迟降低高达 40%，最小化计算开销，并支持并发多模型交互 49。
背景与重要性： 对于生产部署，MCP 解决方案必须能够高效处理负载并根据需求进行扩展。
Stdio 与 HTTP 传输的性能权衡： Stdio 传输由于没有网络开销，通常提供极低的延迟（微秒级），非常适合本地集成 24。然而，它的可伸缩性仅限于单台机器，并且需要主机管理服务器进程的生命周期。相比之下，HTTP 传输（尤其是 Streamable HTTP 和 SSE）专为远程通信而设计，可以利用标准的 Web 扩展技术（如负载均衡、微服务架构）来实现更高的可伸缩性 50。虽然 HTTP 传输会引入网络延迟，但通过优化（如高效序列化、响应流、缓存）可以将其最小化 48。因此，选择传输方式时需要在延迟、吞吐量、部署复杂性和可伸缩性需求之间进行权衡。对于需要极低延迟且交互在本地的场景，Stdio 是理想选择。对于需要跨网络访问、共享服务或大规模部署的场景，HTTP 传输更为合适，尽管需要更仔细地进行性能优化和安全配置。
6.2.1. 缓存策略 (工具列表、资源内容)
数据点： 客户端可以缓存工具列表 3。服务器可以实现智能缓存策略 48，例如 LRU 缓存、分布式缓存、设置缓存过期策略 53。
背景与重要性： 减少冗余计算和网络流量，提高响应速度。
多级缓存和主动失效： 除了客户端缓存工具列表之外，MCP 服务器本身也可以实现多级缓存策略来优化性能。例如，对于频繁请求的只读资源，服务器可以在内存中缓存其内容（例如使用 LRU 策略 53）。对于需要从外部系统（如数据库或 API）获取的数据，服务器可以缓存这些外部调用的结果。为了处理动态数据，服务器不仅需要设置智能的缓存过期策略，还可以实现主动的缓存失效机制。例如，如果底层数据源发生更改，服务器可以主动清除相关缓存条目，并可能通过 MCP 通知（如自定义的资源更改通知）告知客户端该资源已更新，提示客户端刷新其可能存在的本地副本或重新请求该资源。这种结合了客户端和服务器端缓存以及主动失效通知的策略，可以在保持数据相对新鲜的同时最大限度地提高性能。
6.2.2. 异步处理与并发
数据点： 服务器端工具实现通常是异步的 29。使用 asyncio 进行非阻塞 I/O 53。
背景与重要性： 确保服务器在高负载下保持响应，有效利用资源。
任务队列和后台工作线程： 对于 MCP 服务器中特别耗时或资源密集型的工具调用，仅仅使用 async/await 可能不足以防止对主事件循环的长时间占用或确保公平的资源分配。在这种情况下，可以采用更高级的异步处理模式，例如将这些重量级任务卸载到专用的后台工作线程池或任务队列（如 Celery for Python, BullMQ for Node.js）。当客户端调用这类工具时，服务器可以立即返回一个任务 ID 和一个“处理中”的状态（可能通过 MCP 的进度报告机制），然后在后台异步处理该任务。客户端可以稍后使用任务 ID 轮询状态或等待服务器通过通知发送最终结果。这种架构不仅提高了服务器的并发处理能力和响应性，还增强了系统的容错性，因为任务执行与主请求/响应循环解耦。
6.2.3. 消息大小与序列化效率
数据点： 监控消息大小 43。最小化有效负载，使用高效序列化技术 48。
背景与重要性： 大型消息会增加网络延迟和处理开销。
选择性上下文更新和摘要： 当工具返回大量数据或 LLM 对话历史变长时，将所有信息都包含在后续的 LLM 上下文中可能会导致性能下降并超出模型的上下文窗口限制。MCP 客户端（主机）可以采用“选择性上下文更新”策略，即仅将工具结果中最关键的部分或用户明确选择的部分包含到 LLM 上下文中 52。此外，对于冗长的工具输出，主机或专门的 MCP 工具可以先对结果进行“摘要”，然后再将其提供给 LLM 52。这些技术有助于管理上下文大小，减少发送给 LLM 的数据量，从而提高序列化/反序列化效率和整体交互性能，同时确保 LLM 获得最相关的信息。
6.2.4. 横向扩展策略 (负载均衡、微服务)
数据点： Node.js 服务器可以实现负载均衡，使用微服务架构，利用容器化（Docker, Kubernetes）50。Kubernetes 负载均衡、HAProxy、NGINX 可用于 MCP 服务器性能 54。
背景与重要性： 对于高流量的远程 MCP 服务器，单实例可能成为瓶颈。
有状态与无状态 MCP 服务器的扩展： MCP 连接本质上是有状态的（例如，会话控制、能力协商结果、资源订阅）。当对 MCP 服务器进行横向扩展时（例如，在 Kubernetes 中运行多个实例并通过负载均衡器分发流量），需要仔细考虑如何管理这种状态。
无状态服务器设计： 如果可能，将 MCP 服务器设计为尽可能无状态，并将任何必要的会话状态存储在外部共享存储中（如 Redis 或数据库）。这样，任何服务器实例都可以处理来自任何客户端的请求，简化了负载均衡。
有状态服务器与粘性会话： 如果服务器必须在本地维护会话状态，则负载均衡器需要配置“粘性会话”（会话亲和性），以确保来自特定客户端的所有请求都路由到同一服务器实例。这对于依赖于先前交互的 MCP 功能（如某些类型的采样或长时间运行的工具的进度更新）可能至关重要。
Streamable HTTP 中的会话ID： MCP 的 Streamable HTTP 传输规范提到了会话 ID 22，这可以用于在分布式环境中识别和恢复会话，有助于实现更复杂的有状态服务扩展。 选择哪种方法取决于 MCP 服务器提供的具体功能及其状态管理需求。无状态设计通常更易于扩展和管理，但某些 MCP 功能可能天然更适合有状态的实现。
6.3. 调试与监控
数据点： 使用 MCP Inspector 7。Claude Desktop 提供开发者工具，可查看日志、使用 Chrome DevTools 检查网络流量和消息 43。服务器应实现自定义日志记录，跟踪错误，监控性能 43。结构化日志记录，跟踪请求 ID 43。Prometheus 和 Grafana 可用于 MCP 服务器性能监控 54。OpenTelemetry (OTel) 被提议用于 MCP 服务器的追踪信息回传 11。
背景与重要性： 有效的调试和监控工具对于识别问题、分析性能和确保 MCP 系统的可靠性至关重要。
分布式追踪与 OpenTelemetry (OTel)： 随着 MCP 应用变得越来越复杂，涉及多个 MCP 服务器和内部微服务，理解请求的完整生命周期和识别瓶颈变得具有挑战性。OpenTelemetry (OTel) 作为一种开放的、与供应商无关的遥测数据（追踪、指标、日志）标准，正在被提议用于 MCP 生态系统 11。具体而言，一项提议建议 MCP 服务器能够将其内部 OTel 追踪跨度（trace spans）通过 MCP 通知发送回调用客户端。这种方法有几个优点：它简化了服务器的实现，因为服务器不需要配置 OTel 导出器或处理外部可观察性后端的身份验证；它将控制权交给了客户端，客户端可以决定如何以及在何处处理或转发接收到的追踪数据；它利用现有的 MCP 连接，避免了额外的网络配置。通过将来自不同 MCP 服务器和主机应用程序的追踪数据关联起来（可能通过传播 W3C TraceContext），开发人员可以获得端到端的可观察性，从而更容易调试分布式 MCP 工作流、分析性能瓶颈并理解整个系统的行为。这对于维护和优化生产中的复杂 MCP 系统至关重要。
6.4. MCP 的未来方向与演进
数据点： MCP 规范仍在发展，一些功能（如流式输出、身份验证）仍在完善中 4。设计用于未来可扩展性，并保持向后兼容性 14。GitHub Discussions 用于讨论协议演进 11。AWS 提出的增强建议包括人机交互（elicitation）、流式部分结果、增强能力发现、异步通信支持 33。关于无状态 MCP 的讨论，以更好地支持无服务器部署 55。
背景与重要性： 作为一个相对较新的标准，MCP 预计将继续发展以满足 AI 生态系统的新需求。
向更丰富的代理间通信和工作流协调发展： 当前的 MCP 主要关注 AI 应用程序（主机/客户端）与外部能力（服务器）之间的交互。然而，随着 AI 代理变得越来越复杂和自主，对更丰富的代理间 (agent-to-agent) 通信和协调机制的需求日益增长。AWS 提出的增强建议 33，如“人机交互”（elicitation，允许服务器向最终用户请求更多信息）、“流式部分结果”（用于长时运行任务）、“增强能力发现”（工具声明其输出模式和元数据）以及“异步通信”（简化共享状态和轮询驱动的状态检查），都指向了这一方向。这些提议旨在使 MCP 能够支持更复杂的、多步骤的、可能是分布式的代理工作流。例如，一个 MCP 服务器本身可能是一个复杂的代理，它需要调用其他 MCP 服务器（或其他代理）来完成其任务。增强的能力发现将使代理能够更好地理解彼此的能力和预期输出，而异步通信和流式结果则有助于管理长时间运行的、非阻塞的交互。这些演进方向表明 MCP 不仅仅是一个简单的工具调用协议，而是有潜力成为构建复杂、协作式 AI 代理系统的基础通信层。
7. 结论与展望
模型上下文协议 (MCP) 自其推出以来，凭借其清晰的愿景和解决 AI 集成核心痛点的能力，已在人工智能领域引起了广泛关注并获得了显著的早期采纳。它通过提供一个标准化的接口，极大地简化了大型语言模型 (LLM) 与外部工具、数据源和服务之间的交互，将复杂的 M x N 集成问题转变为更易于管理的 M+N 模式 2。
MCP 的核心架构围绕主机、客户端和服务器这三个关键组件构建，通过基于 JSON-RPC 2.0 的协议层和灵活的传输层（包括 Stdio 和 Streamable HTTP）进行通信 15。其提供的基本能力——工具、资源、提示和采样——为扩展 LLM 的功能、为其提供上下文数据以及引导更复杂的交互提供了坚实的基础 6。
官方对多种主流编程语言（如 Python, TypeScript, Java, C#）的 SDK 支持 2，以及由行业领导者（如 Anthropic, OpenAI, Google DeepMind, Microsoft）的采纳和贡献 2，共同推动了 MCP 生态系统的快速发展。这不仅降低了开发者的入门门槛，也增强了社区对该协议长期可行性的信心。
在安全性方面，MCP 从设计之初就强调用户同意与控制、数据隐私和工具安全 13。其授权框架（尤其针对 HTTP 传输）借鉴了 OAuth 2.1 等成熟标准，并针对性地提出了如禁止令牌传递等关键安全原则 25，旨在构建一个值得信赖的交互环境。然而，安全是一个持续的、多层面协同的过程，需要主机、客户端和服务器开发者共同努力，严格遵守最佳实践。
性能和可伸缩性是 MCP 在实际应用中面临的关键考量。通过合理的缓存策略、异步处理、高效的消息序列化以及针对不同部署场景（本地 Stdio 或远程 HTTP）的架构选择，可以有效地优化 MCP 系统的性能 3。对于大规模部署，横向扩展和负载均衡等标准技术同样适用。
展望未来，MCP 仍处于不断发展和完善的过程中。社区讨论和提案（例如关于 OpenTelemetry 集成以增强可观察性 11，以及对更复杂代理工作流和异步通信模式的支持 33）预示着 MCP 将朝着支持更强大、更灵活、更易于管理的 AI 系统集成方向演进。随着 AI 代理能力的不断增强和应用场景的日益复杂化，MCP 作为连接这些智能体与广阔数字世界的“通用端口”，其重要性将愈发凸显。
对于希望构建下一代 AI 应用和服务的开发者和组织而言，深入理解并积极采用 MCP，不仅能够应对当前的集成挑战，更是在为未来更加智能和互联的 AI 生态系统奠定坚实的基础。MCP 的成功将取决于持续的社区参与、标准演进以及在实践中不断涌现的最佳实践和创新应用。
引用的著作
en.wikipedia.org, 访问时间为 六月 1, 2025， https://en.wikipedia.org/wiki/Model_Context_Protocol#:~:text=The%20Model%20Context%20Protocol%20(MCP,%2C%20systems%2C%20and%20data%20sources.
Model Context Protocol - Wikipedia, 访问时间为 六月 1, 2025， https://en.wikipedia.org/wiki/Model_Context_Protocol
Model context protocol (MCP) - OpenAI Agents SDK, 访问时间为 六月 1, 2025， https://openai.github.io/openai-agents-python/mcp/
Model Context Protocol (MCP) - Aisera, 访问时间为 六月 1, 2025， https://aisera.com/blog/mcp-model-context-protocol/
Model Context Protocol (MCP): Integrating Azure OpenAI for Enhanced Tool Integration and Prompting | Microsoft Community Hub, 访问时间为 六月 1, 2025， https://techcommunity.microsoft.com/blog/azure-ai-services-blog/model-context-protocol-mcp-integrating-azure-openai-for-enhanced-tool-integratio/4393788
Model Context Protocol (MCP) Explained - Humanloop, 访问时间为 六月 1, 2025， https://humanloop.com/blog/mcp
Model Context Protocol: Introduction, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/introduction
Learn about MCP in 3 minutes - Replit Docs, 访问时间为 六月 1, 2025， https://docs.replit.com/tutorials/mcp-in-3
Key Concepts and Terminology - Hugging Face MCP Course, 访问时间为 六月 1, 2025， https://huggingface.co/learn/mcp-course/unit1/key-concepts
Model Context Protocol: Discover the missing link in AI integration - Red Hat, 访问时间为 六月 1, 2025， https://www.redhat.com/en/blog/model-context-protocol-discover-missing-link-ai-integration
[Proposal] Adding OpenTelemetry Trace Support to MCP #269 - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol/specification/discussions/269
modelcontextprotocol/servers: Model Context Protocol ... - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol/servers
Specification - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/specification/2025-03-26
Architecture - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/specification/draft/architecture/index
Overview - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/specification/2025-03-26/basic
Core architecture - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/docs/concepts/architecture
The Model Context Protocol: An Emerging Standard for AI Agent-Tool Interactions | Docker, 访问时间为 六月 1, 2025， https://www.docker.com/resources/the-model-context-protocol-white-paper/
Model Context Protocol - Zed, 访问时间为 六月 1, 2025， https://zed.dev/docs/assistant/model-context-protocol
What Is the Model Context Protocol (MCP) and How It Works - Descope, 访问时间为 六月 1, 2025， https://www.descope.com/learn/post/mcp
Cody supports additional context through Anthropic's Model Context Protocol - Sourcegraph, 访问时间为 六月 1, 2025， https://sourcegraph.com/blog/cody-supports-anthropic-model-context-protocol
Welcome to the Copilot Studio Model Context Protocol community, 访问时间为 六月 1, 2025， https://community.powerplatform.com/forums/thread/details/?threadid=cff73983-a110-f011-9989-6045bdeb8a5d
Transports - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/specification/draft/basic/transports
MCP Transport Methods: A Complete Guide for Developers - BytePlus, 访问时间为 六月 1, 2025， https://www.byteplus.com/en/topic/541195
MCP - Protocol Mechanics and Architecture | Pradeep Loganathan's Blog, 访问时间为 六月 1, 2025， https://pradeepl.com/blog/model-context-protocol/mcp-protocol-mechanics-and-architecture/
Security Best Practices - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/specification/draft/basic/security_best_practices
Authorization - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/specification/draft/basic/authorization
modelcontextprotocol/typescript-sdk: The official Typescript ... - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol/typescript-sdk
Tools - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/docs/concepts/tools
For Server Developers - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/quickstart/server
Model Context Protocol (MCP): Revolutionizing AI Development with Seamless Integration, 访问时间为 六月 1, 2025， https://www.cybage.com/blog/model-context-protocol-mcp-revolutionizing-ai-development-with-seamless-integration
i-am-bee/mcp-typescript-sdk: The official Typescript SDK for ... - GitHub, 访问时间为 六月 1, 2025， https://github.com/i-am-bee/mcp-typescript-sdk
smithery-ai/mcp-ts-sdk: The official Typescript SDK for Model Context Protocol servers and clients - GitHub, 访问时间为 六月 1, 2025， https://github.com/smithery-ai/mcp-ts-sdk
Open Protocols for Agent Interoperability Part 1: Inter-Agent Communication on MCP - AWS, 访问时间为 六月 1, 2025， https://aws.amazon.com/blogs/opensource/open-protocols-for-agent-interoperability-part-1-inter-agent-communication-on-mcp/
tzolov/spring-ai-mcp: Java SDK for the Model Context Protocol (MCP), providing seamless integration between Java and Spring applications and MCP-compliant AI models and tools. - GitHub, 访问时间为 六月 1, 2025， https://github.com/tzolov/spring-ai-mcp
modelcontextprotocol/java-sdk: The official Java SDK for ... - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol/java-sdk
modelcontextprotocol/csharp-sdk: The official C# SDK for ... - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol/csharp-sdk
Model Context Protocol - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol
modelcontextprotocol/python-sdk: The official Python SDK ... - GitHub, 访问时间为 六月 1, 2025， https://github.com/modelcontextprotocol/python-sdk
Overview - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/sdk/java/mcp-overview
MCPSharp: The Ultimate .NET Library for Building MCP Servers and Clients #185 - GitHub, 访问时间为 六月 1, 2025， https://github.com/orgs/modelcontextprotocol/discussions/185
For Client Developers - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/quickstart/client
Building MCP with LLMs - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/tutorials/building-mcp-with-llms
Debugging - Model Context Protocol, 访问时间为 六月 1, 2025， https://modelcontextprotocol.io/docs/tools/debugging
Model Context Protocol (MCP) server - Langflow Documentation, 访问时间为 六月 1, 2025， https://docs.langflow.org/mcp-server
Getting Started with Model Context Protocol (MCP) - Auth0, 访问时间为 六月 1, 2025， https://auth0.com/docs/get-started/mcp/getting-started-with-model-context-protocol-mcp
cyanheads/model-context-protocol-resources: Exploring the Model Context Protocol (MCP) through practical guides, clients, and servers I've built while learning about this new protocol. - GitHub, 访问时间为 六月 1, 2025， https://github.com/cyanheads/model-context-protocol-resources
MCP Tools vs Official MCP Inspector: Choosing the Right Tool for Model Context Protocol Development - fka.dev, 访问时间为 六月 1, 2025， https://blog.fka.dev/blog/2025-03-27-mcp-inspector-vs-mcp-tools/
MCP HTTP/SSE: Guide to AI Integration & Scalability - BytePlus, 访问时间为 六月 1, 2025， https://www.byteplus.com/en/topic/541197
MCP Framework: Guide to Model Context Protocol & AI Integration - BytePlus, 访问时间为 六月 1, 2025， https://www.byteplus.com/en/topic/541175
MCP Node.js Implementation: A Complete Guide - BytePlus, 访问时间为 六月 1, 2025， https://www.byteplus.com/en/topic/541240
A Beginner's Guide to Visually Understanding MCP Architecture - Snyk, 访问时间为 六月 1, 2025， https://snyk.io/articles/a-beginners-guide-to-visually-understanding-mcp-architecture/
agent-architectures-with-mcp - AIXplore - Tech Articles - Obsidian Publish, 访问时间为 六月 1, 2025， https://publish.obsidian.md/aixplore/AI+Systems+%26+Architecture/agent-architectures-with-mcp
Model Context Protocol Server Setup Guide | Step-by-Step Tutorial - BytePlus, 访问时间为 六月 1, 2025， https://www.byteplus.com/en/topic/541333
MCP Performance Optimization Techniques: Best Practices for 2025 - BytePlus, 访问时间为 六月 1, 2025， https://www.byteplus.com/en/topic/541336
01-high-level-summary.md - GitHub Gist, 访问时间为 六月 1, 2025， https://gist.github.com/lmmx/8e7adebc6f04cd02d7bdf39668f6f5bb
